{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING BERT FOR SENTIMENT ANALYSIS ON IMDB MOVIE REVIEWS: A METHODOLOGICAL APPROACH**"
      ],
      "metadata": {
        "id": "_i27u-qA5ptE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Introduction**\n",
        "LLMs, including BERT, continue to revolutionize the domain of NLP and, generally, natural language understanding. BERT, which is developed by Google, uses a transformer architecture for pre-training of deep bidirectional representations (Roccabruna et al., 2022). This means that it is able to the left and thereby improving its chances of comprehending complex contextual meanings from a word. The main focus of this task is to perform sentiment analysis on the movie review data taken from IMDb using the IMDb movie reviews dataset, in which the model should work as a BERT-based model to understand the content of the text and be able to classify its sentiment accurately.\n",
        "\n"
      ],
      "metadata": {
        "id": "b0ksUTvQ4zXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Literature review **\n",
        "Prottasha et al., (2022) explored that the quantity of information provided by users through a variety of sources has increased as a result of the Internet connection (Nguyen et al., 2020). Therefore, it should come as no surprise that significant plurality in terms of life experiences and worldviews is advantageous for sentiment analysis. However, the primary problem facing the Bangla natural language processing (NLP) community, which include sentiment analysis, is the absence of labeled data (Geetha and Renuka, 2021). Deep learning methods such as Word2Vec, GloVe, and fastText, which focus primarily on context-free word vectors, are used in much of the Bangla literature. All the same, these models provide every word its own distinct vector. NLP has dramatically improved in recent years, largely because to pre-trained language models like Bert.\n",
        "Nugroho et al., (2021) researched that the one of the key measures to evaluate the effectiveness of mobile apps is the feedback of their users (Pota et al., 2022). Owing to their high complexity levels, textual user evaluations pose large challenges for sentiment analysis because they are within the unstructured data (Li et al., 2021). Prior methods do not take care of contextual information, as found in the reviews. Furthermore, due to the small amount of data, the model gets overfit (Pota et al., 2020). More specifically, BERT is a novel approach to transfer learning that amplifies the negative impact of contextual representation (Qasim et al., 2022). This work adjusts two different pre-trained models to investigate the efficacy of fine-tuning BERT for sentiment analysis.\n"
      ],
      "metadata": {
        "id": "aVO-QnVg4568"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Methodology**\n",
        "This assignment affords an opportunity to use and fine-tune a BERT model for sentiment classification. The procedure consists of several crucial stages: The process consists of a few key steps:\n",
        "1.\tDataset Preparation: The IMDb dataset is used, which contains movie reviews and ratings. The data collection is suitable for this type of tokenization.\n",
        "2.\tTokenization: The task of tokenization is to take raw text and transform it into a form that the model is going to accept (Nguyen et al., 2020). Another preprocessing step practiced is the tokenizer, which is used to divide the text into tokens so they fit the input specification of the model.\n",
        "3.\tModel Configuration: It select a BERT model, known as a transformer that has previously undergone sequence classification training. This model is a good fit for the sentiment analysis problem because it has two possible output labels: positive and negative.\n",
        "4.\tTraining: The model is trained on eighty percent of the IMDb dataset, whereas validation is performed on the remaining twenty percent of the dataset. Hyper-parameters like learning rate, the number of batches or iterations, and epochs are set to achieve the best model.\n",
        "5.\tEvaluation: The last step in training the model is to analyze its performance on a data set that it has not previously encountered. It is often important to know how well the model generalizes to unseen data, and therefore such measures as accuracy and loss are used to determine this.\n"
      ],
      "metadata": {
        "id": "8I4O33ep48u0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvjMp-c6sRM8",
        "outputId": "3bd5b3f1-a203-4dcf-eb19-2a37d9d1bd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: xxhash, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding, pipeline\n",
        "# Import evaluate library for metrics calculation\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "twQhpCCQsVd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "Gi3avoQ8sXCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Result and Discussion**\n",
        "**4.1 BERT Tokenization and Model Preparation**\n",
        "The BERT tokenizer is being fine-tuned via the tokenize_function, which performs the text truncation for the dataset (Geetha and Renuka, 2021). For the tokenization of the dataset with dynamic padding, DataCollatorWithPadding is used next. The BERT model, which is used for sequence classification, is pretrained with two classes: the positive class and the negative class. The dataset is further divided into a training set and a testing set for training as well as for assessing the model."
      ],
      "metadata": {
        "id": "vmawXV6c5R5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample only 100 rows from the training and test sets\n",
        "train_subset = ds[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "test_subset = ds[\"test\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_train = train_subset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_subset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data Collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Load the BERT model for sequence classification (2 classes: positive, negative)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5R-_HF5schl",
        "outputId": "ae649a76-e447-4445-a258-1138e2f75157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n"
      ],
      "metadata": {
        "id": "B43tlMYhsg5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Configuration of Training Parameters**\n",
        "The TrainingArguments class is used to set the positional arguments for training and implement training configurations, such as the output directory for results and logs. Those hyperparameters will be as follows: they are going to be trained and evaluated at the end of each epoch, with the learning rate set at 2e-5 and using batches of 8 samples for training and evaluating (Azhar and Khodra, 2020). It is planned to train the model for 3 epochs with a weight decay of 0.01 to reduce overfitting, and they decided that the training log will be executed every 10 steps.\n"
      ],
      "metadata": {
        "id": "6LXBrmbQ5OdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments (remove compute_metrics from here)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # output directory\n",
        "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
        "    learning_rate=2e-5,              # learning rate\n",
        "    per_device_train_batch_size=2,   # batch size for training\n",
        "    per_device_eval_batch_size=2,    # batch size for evaluation\n",
        "    num_train_epochs=1,              # number of training epochs\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir=\"./logs\",            # directory for storing logs\n",
        "    logging_steps=5,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer (pass compute_metrics here)\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_train,       # training dataset\n",
        "    eval_dataset=tokenized_test,         # evaluation dataset\n",
        "    tokenizer=tokenizer,                 # tokenizer for data preprocessing\n",
        "    data_collator=data_collator,         # data collator for dynamic padding\n",
        "    compute_metrics=compute_metrics     # Pass compute_metrics to the Trainer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5XndfJttn1S",
        "outputId": "7df2caba-17bd-4556-b899-2f8125ebde1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Trainer Initialization and Model Training**\n",
        "The Trainer class takes as parameters the trained BERT model, the arguments for training, and the data for training and evaluation (Pota et al., 2021). The tokenizer is given to help with processing, and the data collator is for dynamic padding. The model is then trained using the trainer. Below is a view of an example of the trainer used to train the model: train () method, through which it trains the model depending upon the training configuration.\n"
      ],
      "metadata": {
        "id": "1wQWaeRM5LMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n",
        "print(f\"Test Loss: {results['eval_loss']:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine-tuned-bert-imdb\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bert-imdb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "z2kJKrNWtpzx",
        "outputId": "8583e0ea-f1e3-4352-fd42-03aa935891d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 01:18, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.837100</td>\n",
              "      <td>0.730052</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine-tuned-bert-imdb/tokenizer_config.json',\n",
              " './fine-tuned-bert-imdb/special_tokens_map.json',\n",
              " './fine-tuned-bert-imdb/vocab.txt',\n",
              " './fine-tuned-bert-imdb/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 Model Evaluation, Saving, and Inference**\n",
        "The section first assesses the model using trainer.evaluate()  and then prints the accuracy of the test and the loss. The fine-tuned model and tokenizer are then saved in the directory. /fine-tuned-bert-imdb. For inference, the sentiment analysis pipeline function is used to load the model. The model is tested with example reviews, and what is more, the outcomes provoking the sentiment analysis are given with the sentiment label and the degree of confidence.\n"
      ],
      "metadata": {
        "id": "DGIr7U9A5Go2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model for inference\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"./fine-tuned-bert-imdb\", tokenizer=tokenizer)\n",
        "\n",
        "# Test the model with some example reviews\n",
        "examples = [\n",
        "    \"I absolutely loved this movie. The acting was great and the storyline was touching.\",\n",
        "    \"This was the worst movie I've ever seen. It was a complete waste of time.\"\n",
        "]\n",
        "\n",
        "results = sentiment_analysis(examples)\n",
        "\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Review: {examples[i]}\")\n",
        "    print(f\"Sentiment: {result['label']}, Confidence: {result['score']:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bajVcyFxHcS",
        "outputId": "e7ba9d44-6f31-4ff1-e9cf-040845a298f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: I absolutely loved this movie. The acting was great and the storyline was touching.\n",
            "Sentiment: LABEL_0, Confidence: 0.5890\n",
            "\n",
            "Review: This was the worst movie I've ever seen. It was a complete waste of time.\n",
            "Sentiment: LABEL_0, Confidence: 0.5755\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Conclusion**\n",
        "This research effectively illustrates to optimize a BERT model for sentiment analysis when taken as a whole (Qasim et al., 2022). The pre-trained feature of BERT is used by the classifier to determine if a text is sentimental, and it is adjusted based on task requirements. This experiment demonstrates the applicability of LLMs in various NLP scenarios and emphasizes the significance of fine-tuning to customize models for particular tasks and datasets.\n"
      ],
      "metadata": {
        "id": "B7jPdDrd5W0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference **\n",
        "Azhar, A.N. and Khodra, M.L., 2020, September. Fine-tuning pretrained multilingual BERT model for Indonesian aspect-based sentiment analysis. In 2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA) (pp. 1-6). IEEE.\n",
        "Geetha, M.P. and Renuka, D.K., 2021. Improving the performance of aspect based sentiment analysis using fine-tuned Bert Base Uncased model. International Journal of Intelligent Networks, 2, pp.64-69.\n",
        "Li, X., Wang, X. and Liu, H., 2021, May. Research on fine-tuning strategy of sentiment analysis model based on BERT. In 2021 international conference on communications, information system and computer engineering (CISCE) (pp. 798-802). IEEE.\n",
        "Nguyen, Q.T., Nguyen, T.L., Luong, N.H. and Ngo, Q.H., 2020, November. Fine-tuning bert for sentiment analysis of vietnamese reviews. In 2020 7th NAFOSTED conference on information and computer science (NICS) (pp. 302-307). IEEE.\n",
        "Nugroho, K.S., Sukmadewa, A.Y., Wuswilahaken DW, H., Bachtiar, F.A. and Yudistira, N., 2021, September. Bert fine-tuning for sentiment analysis on indonesian mobile apps reviews. In Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology (pp. 258-264).\n",
        "Pota, M., Ventura, M., Catelli, R. and Esposito, M., 2020. An effective BERT-based pipeline for Twitter sentiment analysis: A case study in Italian. Sensors, 21(1), p.133.\n",
        "Pota, M., Ventura, M., Fujita, H. and Esposito, M., 2021. Multilingual evaluation of pre-processing for BERT-based sentiment analysis of tweets. Expert Systems with Applications, 181, p.115119.\n",
        "Prottasha, N.J., Sami, A.A., Kowsher, M., Murad, S.A., Bairagi, A.K., Masud, M. and Baz, M., 2022. Transfer learning for sentiment analysis using BERT based supervised fine-tuning. Sensors, 22(11), p.4157.\n",
        "Qasim, R., Bangyal, W.H., Alqarni, M.A. and Ali Almazroi, A., 2022. A Fineâ€Tuned BERTâ€Based Transfer Learning Approach for Text Classification. Journal of healthcare engineering, 2022(1), p.3498123.\n",
        "Roccabruna, G., Azzolin, S. and Riccardi, G., 2022. Multi-source multi-domain sentiment analysis with BERT-based models. In European Language Resources Association (pp. 581-589). European Language Resources Association.\n"
      ],
      "metadata": {
        "id": "y3Hr9lni5Z7k"
      }
    }
  ]
}